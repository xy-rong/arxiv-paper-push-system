#!/usr/bin/env python3
"""
arXiv è®ºæ–‡æ¨é€ç³»ç»Ÿ - æ¼”ç¤ºç‰ˆæœ¬
ä¸éœ€è¦OpenAI APIï¼Œä½¿ç”¨è®ºæ–‡æ‘˜è¦ä½œä¸ºæ€»ç»“
"""

import argparse
from typing import List
from datetime import datetime

from arxiv_fetcher import ArxivFetcher
from report_generator import ReportGenerator
import config

class ArxivPusherDemo:
    def __init__(self):
        self.fetcher = ArxivFetcher(max_results=config.MAX_RESULTS)
        self.reporter = ReportGenerator()
    
    def process_papers_with_abstract_summary(self, papers: List[dict]) -> List[dict]:
        """ä½¿ç”¨è®ºæ–‡æ‘˜è¦ä½œä¸ºæ€»ç»“ï¼ˆæˆªå–å‰200å­—ç¬¦ï¼‰"""
        for paper in papers:
            abstract = paper['abstract']
            # æ¸…ç†å¹¶æˆªå–æ‘˜è¦
            clean_abstract = self.fetcher.clean_text(abstract)
            if len(clean_abstract) > 200:
                summary = clean_abstract[:200] + "..."
            else:
                summary = clean_abstract
            paper['summary'] = summary
        return papers
    
    def fetch_and_process(self, keywords: List[str], days_back: int = 1, 
                         output_format: str = "html", save_file: bool = True) -> List[dict]:
        """è·å–å¹¶å¤„ç†è®ºæ–‡"""
        
        print(f"ğŸ” æ­£åœ¨æœç´¢å…³é”®è¯: {', '.join(keywords)}")
        print(f"ğŸ“… æœç´¢æœ€è¿‘ {days_back} å¤©çš„è®ºæ–‡...")
        
        # è·å–è®ºæ–‡
        papers = self.fetcher.search_papers(keywords, days_back)
        
        if not papers:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
            return []
        
        print(f"ğŸ“š æ‰¾åˆ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡")
        
        # ä½¿ç”¨æ‘˜è¦ä½œä¸ºæ€»ç»“
        papers = self.process_papers_with_abstract_summary(papers)
        
        # ç”ŸæˆæŠ¥å‘Š
        if save_file:
            report_path = self.reporter.save_report(papers, keywords, output_format)
            print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        # åœ¨æ§åˆ¶å°æ˜¾ç¤º
        self.reporter.print_console_report(papers, keywords)
        
        return papers
    
    def run_once(self, keywords: List[str], days_back: int = 1, 
                output_format: str = "html", save_file: bool = True):
        """è¿è¡Œä¸€æ¬¡æ¨é€"""
        try:
            papers = self.fetch_and_process(keywords, days_back, output_format, save_file)
            print(f"\nâœ… æ¨é€å®Œæˆ! å¤„ç†äº† {len(papers)} ç¯‡è®ºæ–‡")
        except Exception as e:
            print(f"âŒ æ¨é€è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

def main():
    parser = argparse.ArgumentParser(description="arXiv è®ºæ–‡æ¨é€ç³»ç»Ÿ - æ¼”ç¤ºç‰ˆæœ¬")
    parser.add_argument(
        "--keywords", "-k", 
        nargs="+", 
        default=config.DEFAULT_KEYWORDS,
        help="æœç´¢å…³é”®è¯"
    )
    parser.add_argument(
        "--days", "-d", 
        type=int, 
        default=config.DAYS_BACK,
        help="æœç´¢æœ€è¿‘å‡ å¤©çš„è®ºæ–‡ (é»˜è®¤: 1)"
    )
    parser.add_argument(
        "--format", "-f", 
        choices=["html", "markdown"], 
        default="html",
        help="è¾“å‡ºæ ¼å¼ (é»˜è®¤: html)"
    )
    parser.add_argument(
        "--no-save", 
        action="store_true",
        help="ä¸ä¿å­˜æŠ¥å‘Šæ–‡ä»¶ï¼Œåªåœ¨æ§åˆ¶å°æ˜¾ç¤º"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨é€å™¨
    pusher = ArxivPusherDemo()
    
    # å•æ¬¡è¿è¡Œæ¨¡å¼
    pusher.run_once(
        keywords=args.keywords,
        days_back=args.days,
        output_format=args.format,
        save_file=not args.no_save
    )

if __name__ == "__main__":
    main()